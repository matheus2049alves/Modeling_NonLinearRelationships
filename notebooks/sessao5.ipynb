{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37e84cd5",
   "metadata": {},
   "source": [
    "## **Parte II: N√£o-Linearidade nos Par√¢metros**\n",
    "*Onde o pr√≥prio modelo √© intrinsecamente n√£o-linear.*\n",
    "\n",
    "---\n",
    "\n",
    "### **üß† Se√ß√£o 5: O Conceito Fundamental e a Conex√£o com Redes Neurais**\n",
    "\n",
    "#### **5.1. O que √© N√£o-Linearidade nos Par√¢metros?**\n",
    "\n",
    "At√© agora, lidamos com a **n√£o-linearidade nas features**. Em todos os casos, o modelo final ainda era uma soma ponderada de termos, ou seja, era *linear em seus par√¢metros* ($\\beta_i$).\n",
    "\n",
    "Por exemplo, no modelo polinomial, a equa√ß√£o √©:\n",
    "$$y = \\beta_0 \\cdot 1 + \\beta_1 \\cdot x + \\beta_2 \\cdot x^2$$\n",
    "Apesar da rela√ß√£o entre $y$ e $x$ ser uma curva, a rela√ß√£o entre $y$ e os par√¢metros $\\beta_0, \\beta_1, \\beta_2$ √© linear. √â por isso que `LinearRegression` ainda funcionava.\n",
    "\n",
    "Agora, entramos em modelos **n√£o-lineares nos par√¢metros**. Isso ocorre quando os par√¢metros que queremos encontrar est√£o \"dentro\" de fun√ß√µes n√£o-lineares, como exponenciais, logaritmos, ou sendo divididos uns pelos outros.\n",
    "\n",
    "O exemplo cl√°ssico √© o do decaimento exponencial:\n",
    "$$y = A \\cdot e^{-kx}$$\n",
    "Nesta equa√ß√£o, √© imposs√≠vel reescrev√™-la como uma soma ponderada dos par√¢metros $A$ e $k$. N√£o podemos usar `LinearRegression` para encontrar seus valores, pois n√£o h√° uma solu√ß√£o matem√°tica direta (uma \"f√≥rmula pronta\"). Para resolver isso, precisamos de **otimizadores iterativos** ‚Äî algoritmos que testam diferentes valores para os par√¢metros de forma inteligente at√© encontrar a combina√ß√£o que minimiza o erro.\n",
    "\n",
    "---\n",
    "#### **5.2. A Conex√£o com Redes Neurais**\n",
    "\n",
    "Esta discuss√£o nos leva diretamente ao cora√ß√£o das **redes neurais**. Uma rede neural √© o exemplo por excel√™ncia de um modelo massivamente n√£o-linear em seus par√¢metros.\n",
    "\n",
    "Pense em um √∫nico neur√¥nio:\n",
    "1.  Primeiro, ele calcula uma soma ponderada: $z = (w_1 x_1 + w_2 x_2 + \\dots) + b$\n",
    "2.  Em seguida, ele aplica uma **fun√ß√£o de ativa√ß√£o n√£o-linear**: $a = \\sigma(z)$\n",
    "\n",
    "A sa√≠da final, $a$, depende dos par√¢metros $w$ (pesos) e $b$ (bias), que est√£o \"presos\" dentro da fun√ß√£o n√£o-linear $\\sigma$ (como a Sigmoid, ReLU, etc.). Quando empilhamos dezenas ou milhares desses neur√¥nios, a n√£o-linearidade do modelo como um todo se torna imensamente complexa.\n",
    "\n",
    "O processo de **treinamento de uma rede neural** nada mais √© do que um processo de otimiza√ß√£o iterativa (geralmente, o *Gradient Descent* ou suas variantes, como o Adam) para encontrar os melhores valores para os milh√µes de par√¢metros $w$ e $b$ que minimizam uma fun√ß√£o de erro (como o Erro Quadr√°tico M√©dio - MSE).\n",
    "\n",
    "Conceitualmente, o que o `scipy.optimize.curve_fit` faz para encontrar 2 ou 3 par√¢metros de um modelo te√≥rico √© an√°logo ao que o `model.fit()` do Keras faz para encontrar milh√µes de par√¢metros em uma rede neural. Ambos s√£o m√©todos para resolver problemas de regress√£o (ou classifica√ß√£o) que s√£o **n√£o-lineares em seus par√¢metros**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
